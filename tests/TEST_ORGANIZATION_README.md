"""
M-Pesa Channel Management - Test Organization Best Practices

Updated for your existing backend/frontend test structure with Docker Compose integration.
"""

# ===========================================
# ğŸ“ YOUR CURRENT PROJECT STRUCTURE
# ===========================================

"""
your-project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”œâ”€â”€ tests/                    # âœ… Your existing backend tests
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ conftest.py
â”‚   â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_mpesa_service.py
â”‚   â”‚   â”‚   â””â”€â”€ test_auth.py
â”‚   â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_api_endpoints.py
â”‚   â”‚   â”‚   â””â”€â”€ test_database.py
â”‚   â”‚   â””â”€â”€ fixtures/
â”‚   â”‚       â”œâ”€â”€ test_data.py
â”‚   â”‚       â””â”€â”€ factories.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”œâ”€â”€ tests/                    # âœ… Your existing frontend tests
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ setup.js
â”‚   â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”‚   â”œâ”€â”€ components.test.js
â”‚   â”‚   â”‚   â””â”€â”€ services.test.js
â”‚   â”‚   â”œâ”€â”€ e2e/
â”‚   â”‚   â”‚   â”œâ”€â”€ workflows.test.js
â”‚   â”‚   â”‚   â””â”€â”€ integration.test.js
â”‚   â”‚   â””â”€â”€ fixtures/
â”‚   â”‚       â””â”€â”€ test-data.js
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”‚
â””â”€â”€ tests/                        # âœ… Your existing shared tests
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ conftest.py
    â”œâ”€â”€ unit/
    â”œâ”€â”€ integration/
    â””â”€â”€ e2e/
"""

# ===========================================
# ğŸ³ DOCKER COMPOSE TEST CONFIGURATION
# ===========================================

# docker-compose.test.yml (at project root)
"""
version: '3.8'

services:
  # Backend test service
  backend-tests:
    build:
      context: ./backend
      dockerfile: Dockerfile
    environment:
      - ENVIRONMENT=test
      - DATABASE_URL=postgresql://test:test@db:5432/test_db
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - db
      - redis
    volumes:
      - ./backend:/app
      - ./tests:/shared-tests
    command: python -m pytest tests/ backend/tests/ -v --tb=short
    networks:
      - test_network

  # Frontend test service
  frontend-tests:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    environment:
      - NODE_ENV=test
      - API_URL=http://backend:8000
    depends_on:
      - backend-tests
    volumes:
      - ./frontend:/app
      - ./tests:/shared-tests
    command: npm test
    networks:
      - test_network

  # Test database
  db:
    image: postgres:15
    environment:
      - POSTGRES_DB=test_db
      - POSTGRES_USER=test
      - POSTGRES_PASSWORD=test
    volumes:
      - test_db_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    networks:
      - test_network

  # Redis for testing
  redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"
    networks:
      - test_network

  # Mock external services
  mock-mpesa:
    build:
      context: ./tests/mock_services
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    networks:
      - test_network

volumes:
  test_db_data:

networks:
  test_network:
    driver: bridge
"""

"""
tests/
â”œâ”€â”€ __init__.py                    # Makes tests a Python package
â”œâ”€â”€ conftest.py                   # Global pytest fixtures and configuration
â”œâ”€â”€ pytest.ini                    # Pytest configuration
â”œâ”€â”€ docker-compose.test.yml       # Test-specific Docker Compose
â”œâ”€â”€ requirements-test.txt         # Test dependencies
â”œâ”€â”€ README.md                     # Testing documentation
â”‚
â”œâ”€â”€ unit/                         # Unit tests (fast, isolated)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py               # Unit-specific fixtures
â”‚   â”œâ”€â”€ test_mpesa_service.py     # Service layer tests
â”‚   â”œâ”€â”€ test_channel_validation.py # Schema validation tests
â”‚   â”œâ”€â”€ test_auth_middleware.py   # Authentication tests
â”‚   â””â”€â”€ test_error_handling.py    # Error handling tests
â”‚
â”œâ”€â”€ integration/                  # Integration tests (slower, broader scope)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py               # Integration-specific fixtures
â”‚   â”œâ”€â”€ test_channel_api.py       # API endpoint tests
â”‚   â”œâ”€â”€ test_database_flow.py     # Database interaction tests
â”‚   â”œâ”€â”€ test_external_apis.py     # External service integration
â”‚   â””â”€â”€ test_auth_flow.py         # Authentication flow tests
â”‚
â”œâ”€â”€ e2e/                         # End-to-end tests (slowest, full stack)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py               # E2E-specific fixtures
â”‚   â”œâ”€â”€ test_channel_onboarding.py # Complete user workflows
â”‚   â”œâ”€â”€ test_simulation_flow.py    # Payment simulation workflows
â”‚   â”œâ”€â”€ test_analytics_flow.py     # Analytics workflows
â”‚   â””â”€â”€ test_error_scenarios.py    # Error handling workflows
â”‚
â”œâ”€â”€ fixtures/                     # Test data and fixtures
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sample_data.py            # Mock data generators
â”‚   â”œâ”€â”€ factories.py              # Model factories
â”‚   â””â”€â”€ database_fixtures.py      # Database test fixtures
â”‚
â””â”€â”€ utils/                        # Test utilities
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_helpers.py           # Common test helpers
    â”œâ”€â”€ mock_services.py          # Mock service implementations
    â””â”€â”€ api_client.py             # Test API client
"""

# ===========================================
# âš™ï¸ PYTEST CONFIGURATION
# ===========================================

# tests/pytest.ini
"""
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --cov=app
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml
    --junitxml=reports/junit.xml
    --html=reports/report.html
markers =
    unit: Fast, isolated unit tests
    integration: Database and external service tests
    e2e: Full user workflow tests
    slow: Tests that take longer than 30 seconds
    docker: Tests that require Docker services
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
"""

# ===========================================
# ğŸ³ DOCKER COMPOSE TEST CONFIGURATION
# ===========================================

# tests/docker-compose.test.yml
"""
version: '3.8'

services:
  # Main application (same as dev but with test config)
  app:
    build:
      context: ..
      dockerfile: Dockerfile
    environment:
      - ENVIRONMENT=test
      - DATABASE_URL=postgresql://test:test@db:5432/test_db
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - db
      - redis
    volumes:
      - ../:/app
    command: python -m pytest tests/ -v --tb=short
    networks:
      - test_network

  # Test database (separate from dev)
  db:
    image: postgres:15
    environment:
      - POSTGRES_DB=test_db
      - POSTGRES_USER=test
      - POSTGRES_PASSWORD=test
    volumes:
      - test_db_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"  # Different port to avoid conflicts
    networks:
      - test_network

  # Redis for testing
  redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"  # Different port to avoid conflicts
    networks:
      - test_network

  # Mock external services for integration tests
  mock-mpesa:
    build:
      context: ../tests/mock_services
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    networks:
      - test_network

volumes:
  test_db_data:

networks:
  test_network:
    driver: bridge
"""

# ===========================================
# ğŸ”§ GLOBAL TEST CONFIGURATION
# ===========================================

# tests/conftest.py
"""
"""
Global pytest fixtures and configuration for all test types.
"""
import pytest
import asyncio
from typing import Generator, AsyncGenerator
from httpx import AsyncClient
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker
from app.core.database import Base
from app.main import app
import os

# Test database configuration
TEST_DATABASE_URL = os.getenv(
    "TEST_DATABASE_URL",
    "postgresql+asyncpg://test:test@localhost:5433/test_db"
)

# Create test database engine
test_engine = create_async_engine(TEST_DATABASE_URL, echo=False)
TestAsyncSession = sessionmaker(
    test_engine, class_=AsyncSession, expire_on_commit=False
)

@pytest.fixture(scope="session")
def event_loop():
    """Create event loop for async tests"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(scope="session", autouse=True)
async def setup_test_database():
    """Create and teardown test database"""
    async with test_engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)
        await conn.run_sync(Base.metadata.create_all)
    yield
    async with test_engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)

@pytest.fixture
async def db_session() -> AsyncGenerator[AsyncSession, None]:
    """Database session for tests"""
    async with TestAsyncSession() as session:
        yield session
        await session.rollback()  # Rollback after each test

@pytest.fixture
async def async_client() -> AsyncGenerator[AsyncClient, None]:
    """HTTP client for testing FastAPI endpoints"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        yield client

@pytest.fixture(autouse=True)
def mock_environment(monkeypatch):
    """Mock environment variables for tests"""
    monkeypatch.setenv("ENVIRONMENT", "test")
    monkeypatch.setenv("SECRET_KEY", "test-secret-key-for-jwt-tokens")
    monkeypatch.setenv("MPEsa_API_URL", "https://sandbox.safaricom.co.ke")

# Test data fixtures
@pytest.fixture
def sample_merchant_data():
    """Sample merchant data for tests"""
    return {
        "business_name": "Test Business",
        "owner_name": "Test Owner",
        "email": "test@example.com",
        "phone": "254712345678",
        "business_type": "retail",
        "mpesa_till_number": "174379"
    }

@pytest.fixture
def sample_channel_data():
    """Sample M-Pesa channel data for tests"""
    return {
        "shortcode": "174379",
        "channel_type": "paybill",
        "environment": "sandbox",
        "consumer_key": "test_consumer_key",
        "consumer_secret": "test_consumer_secret"
    }
"""

# ===========================================
# ğŸ¯ UNIT TEST ORGANIZATION
# ===========================================

# tests/unit/conftest.py
"""
"""
Unit-specific test fixtures and configuration.
"""
import pytest
from unittest.mock import Mock, patch
from app.services.mpesa_service_refactored import MpesaConfig, MpesaServiceFactory


@pytest.fixture
def mpesa_config():
    """Standard M-Pesa configuration for unit tests"""
    return MpesaConfig(
        consumer_key="test_consumer_key",
        consumer_secret="test_consumer_secret",
        environment="sandbox",
        timeout=30,
        max_retries=3,
        retry_delay=0.1
    )

@pytest.fixture
def mock_http_client():
    """Mock HTTP client for unit tests"""
    with patch('httpx.AsyncClient') as mock_client:
        mock_instance = Mock()
        mock_client.return_value = mock_instance
        yield mock_instance

@pytest.fixture
def channel_service(mpesa_config):
    """M-Pesa channel service for unit tests"""
    return MpesaServiceFactory.create_channel_service(
        consumer_key=mpesa_config.consumer_key,
        consumer_secret=mpesa_config.consumer_secret,
        environment=mpesa_config.environment
    )

# Mock external dependencies
@pytest.fixture(autouse=True)
def mock_redis():
    """Mock Redis for unit tests"""
    with patch('app.core.cache.redis_client') as mock_redis:
        yield mock_redis

@pytest.fixture(autouse=True)
def mock_logger():
    """Mock logger for unit tests"""
    with patch('app.services.mpesa_service_refactored.logger') as mock_logger:
        yield mock_logger
"""

# ===========================================
# ğŸ”— INTEGRATION TEST ORGANIZATION
# ===========================================

# tests/integration/conftest.py
"""
"""
Integration-specific test fixtures and configuration.
"""
import pytest
from fastapi.testclient import TestClient
from app.main import app
from sqlalchemy.ext.asyncio import AsyncSession


@pytest.fixture
def test_client():
    """FastAPI test client with mocked authentication"""
    return TestClient(app)

@pytest.fixture
def authenticated_client(test_client):
    """Test client with authentication headers"""
    # Mock authentication middleware
    with patch("app.api.v1.endpoints.auth.oauth2_scheme") as mock_auth:
        mock_auth.return_value = "test_token"

        # Add authorization header
        test_client.headers = {"Authorization": "Bearer test_token"}
        yield test_client

@pytest.fixture
async def db_with_test_data(db_session):
    """Database session with pre-populated test data"""
    # Create test merchant
    from app.models.merchant import Merchant
    from app.models.mpesa_channel import MpesaChannel

    merchant = Merchant(
        business_name="Test Business",
        owner_name="Test Owner",
        email="test@example.com",
        phone="254712345678",
        business_type="retail",
        mpesa_till_number="174379"
    )
    db_session.add(merchant)
    await db_session.commit()
    await db_session.refresh(merchant)

    # Create test channel
    channel = MpesaChannel(
        merchant_id=merchant.id,
        shortcode="174379",
        channel_type="paybill",
        environment="sandbox",
        status="unverified"
    )
    db_session.add(channel)
    await db_session.commit()

    yield db_session

    # Cleanup
    await db_session.rollback()
"""

# ===========================================
# ğŸŒ E2E TEST ORGANIZATION
# ===========================================

# tests/e2e/conftest.py
"""
"""
E2E-specific test fixtures and configuration.
"""
import pytest
from playwright.sync_api import Browser, BrowserContext, Page


@pytest.fixture(scope="session")
def browser_context_args(browser_context_args):
    """Configure browser context for E2E tests"""
    return {
        **browser_context_args,
        "record_video_dir": "test-results/videos/",
        "record_har_path": "test-results/hars/",
        "viewport": {"width": 1280, "height": 720}
    }

@pytest.fixture
def authenticated_page(page: Page):
    """Page with authenticated session"""
    # Mock authentication or login
    page.goto("/login")
    page.fill("#email", "test@example.com")
    page.fill("#password", "test_password")
    page.click("button[type='submit']")
    page.wait_for_url("**/dashboard")
    return page

@pytest.fixture
def clean_database():
    """Ensure clean database state for E2E tests"""
    # Reset database before each test
    # This would require database cleanup utilities
    pass
"""

# ===========================================
# ğŸ“Š TEST EXECUTION STRATEGIES
# ===========================================

# scripts/run_tests.py
"""
"""
Test execution scripts for different scenarios.
"""

import subprocess
import sys
import argparse
from pathlib import Path


def run_unit_tests():
    """Run unit tests only"""
    cmd = [
        sys.executable, "-m", "pytest",
        "tests/unit/",
        "-v",
        "--tb=short",
        "--cov=app.services",
        "--cov-report=term-missing",
        "-m", "unit"
    ]
    subprocess.run(cmd, check=True)


def run_integration_tests():
    """Run integration tests only"""
    cmd = [
        sys.executable, "-m", "pytest",
        "tests/integration/",
        "-v",
        "--tb=short",
        "--cov=app.api",
        "--cov-report=term-missing",
        "-m", "integration"
    ]
    subprocess.run(cmd, check=True)


def run_e2e_tests():
    """Run E2E tests only"""
    cmd = [
        sys.executable, "-m", "pytest",
        "tests/e2e/",
        "-v",
        "--tb=short",
        "--browser=chromium",
        "--headed=false",
        "-m", "e2e"
    ]
    subprocess.run(cmd, check=True)


def run_all_tests():
    """Run all tests with coverage"""
    cmd = [
        sys.executable, "-m", "pytest",
        "tests/",
        "-v",
        "--tb=short",
        "--cov=app",
        "--cov-report=html:htmlcov",
        "--cov-report=xml",
        "--junitxml=reports/junit.xml"
    ]
    subprocess.run(cmd, check=True)


def run_tests_parallel():
    """Run tests in parallel for faster execution"""
    cmd = [
        sys.executable, "-m", "pytest",
        "tests/",
        "-v",
        "-n", "auto",  # Auto-detect CPU cores
        "--tb=short",
        "--cov=app",
        "--cov-report=term-missing"
    ]
    subprocess.run(cmd, check=True)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--type", choices=["unit", "integration", "e2e", "all", "parallel"],
                       default="all", help="Type of tests to run")

    args = parser.parse_args()

    if args.type == "unit":
        run_unit_tests()
    elif args.type == "integration":
        run_integration_tests()
    elif args.type == "e2e":
        run_e2e_tests()
    elif args.type == "parallel":
        run_tests_parallel()
    else:
        run_all_tests()
"""

# ===========================================
# ğŸš€ CI/CD INTEGRATION
# ===========================================

# .github/workflows/tests.yml
"""
name: Run Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
    - name: Run unit tests
      run: python scripts/run_tests.py --type unit

  integration-tests:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: test_db
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
    - name: Run integration tests
      run: python scripts/run_tests.py --type integration
      env:
        DATABASE_URL: postgresql://test:test@localhost:5432/test_db

  e2e-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    - name: Install Playwright
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/requirements-test.txt
        playwright install chromium
    - name: Run E2E tests
      run: python scripts/run_tests.py --type e2e
"""

# ===========================================
# ğŸ“ˆ TEST REPORTING & MONITORING
# ===========================================

# tests/utils/test_report_generator.py
"""
"""
Generate comprehensive test reports.
"""
import pytest
from typing import Dict, Any
import json
from pathlib import Path


class TestReportGenerator:
    """Generate comprehensive test reports"""

    def __init__(self, test_results: Dict[str, Any]):
        self.results = test_results

    def generate_html_report(self, output_path: str):
        """Generate HTML test report"""
        # Implementation would create detailed HTML report
        pass

    def generate_summary_report(self) -> Dict[str, Any]:
        """Generate summary report"""
        return {
            "total_tests": self.results.get("tests", 0),
            "passed": self.results.get("passed", 0),
            "failed": self.results.get("failed", 0),
            "skipped": self.results.get("skipped", 0),
            "coverage": self.results.get("coverage", 0),
            "duration": self.results.get("duration", 0)
        }

    def generate_failure_report(self) -> Dict[str, Any]:
        """Generate detailed failure report"""
        return {
            "failures": self.results.get("failures", []),
            "errors": self.results.get("errors", []),
            "tracebacks": self.results.get("tracebacks", [])
        }
"""

# ===========================================
# ğŸ¯ BEST PRACTICES SUMMARY
# ===========================================

"""
BEST PRACTICES FOR TEST ORGANIZATION:

1. ğŸ“ **Clear Structure**
   - Separate directories for different test types
   - Consistent naming conventions
   - Shared fixtures in conftest.py files

2. âš™ï¸ **Configuration Management**
   - Environment-specific configurations
   - Proper test isolation
   - Resource cleanup

3. ğŸ³ **Docker Integration**
   - Separate test services
   - Proper networking
   - Volume management

4. ğŸš€ **Execution Optimization**
   - Parallel test execution
   - Selective test running
   - Fast feedback loops

5. ğŸ“Š **Monitoring & Reporting**
   - Coverage tracking
   - Performance monitoring
   - Detailed reporting

6. ğŸ”„ **CI/CD Integration**
   - Automated test execution
   - Quality gates
   - Artifact collection

7. ğŸ› ï¸ **Maintainability**
   - Reusable fixtures
   - Clear test documentation
   - Easy test discovery

8. ğŸ“ˆ **Scalability**
   - Independent test execution
   - Resource optimization
   - Parallel processing
"""

# ===========================================
# ğŸ“ IMPLEMENTATION CHECKLIST
# ===========================================

"""
âœ… IMPLEMENTATION CHECKLIST:

1. [ ] Create tests/ directory structure
2. [ ] Set up pytest.ini configuration
3. [ ] Create conftest.py files for each level
4. [ ] Implement unit test fixtures
5. [ ] Implement integration test fixtures
6. [ ] Implement E2E test fixtures
7. [ ] Create docker-compose.test.yml
8. [ ] Set up test requirements
9. [ ] Create test execution scripts
10. [ ] Set up CI/CD workflows
11. [ ] Configure test reporting
12. [ ] Document test organization
13. [ ] Train team on test structure
14. [ ] Set up monitoring and alerts
"""

# ===========================================
# ğŸš€ QUICK START COMMANDS
# ===========================================

"""
QUICK START COMMANDS:

# Run all tests
python scripts/run_tests.py --type all

# Run only unit tests
python scripts/run_tests.py --type unit

# Run with coverage
pytest tests/ --cov=app --cov-report=html

# Run specific test file
pytest tests/unit/test_mpesa_service.py -v

# Run with Docker Compose
docker-compose -f tests/docker-compose.test.yml up --abort-on-container-exit

# Run E2E tests with browser
pytest tests/e2e/ --browser=chromium --headed
"""
